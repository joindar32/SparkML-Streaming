{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfmUDWgt0AhE",
        "outputId": "b7fcbf1f-563a-48a9-971d-ec882c3738b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=78a41ecfc9aed255ca6273536a886bb7307b93bca7ef6c1194d866311824e15f\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml import PipelineModel\n",
        "from pyspark.sql.types import FloatType, IntegerType\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.pipeline import PipelineModel\n",
        "import zipfile\n",
        "import os"
      ],
      "metadata": {
        "id": "k5pBMu8_0G8P"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def apply_model_to_data_string(model_zip_path, data_string):\n",
        "    \"\"\"\n",
        "    Загружает модель машинного обучения из указанного zip-файла и применяет её к данным, поданным в виде строки.\n",
        "\n",
        "    Параметры:\n",
        "    - model_zip_path (str): Путь к zip-файлу, содержащему сохраненную модель машинного обучения.\n",
        "    - data_string (str): Строка с данными, разделенными запятыми.\n",
        "    \"\"\"\n",
        "\n",
        "    with zipfile.ZipFile(model_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(\"unzipped_model\")\n",
        "\n",
        "    # Инициализация Spark Session\n",
        "    spark = SparkSession.builder.appName(\"ModelUsageApp\").getOrCreate()\n",
        "\n",
        "\n",
        "    model_path = os.path.join(\"unzipped_model\", \"lr_model\")\n",
        "    model = PipelineModel.load(model_path)\n",
        "\n",
        "    # Преобразование данных из строки в формат DataFrame\n",
        "    data_list = data_string.split(\",\")\n",
        "    data_dict = {\n",
        "        \"battery_power\": int(data_list[0]),\n",
        "        \"blue\": int(data_list[1]),\n",
        "        \"clock_speed\": float(data_list[2]),\n",
        "        \"dual_sim\": int(data_list[3]),\n",
        "        \"fc\": int(data_list[4]),\n",
        "        \"four_g\": int(data_list[5]),\n",
        "        \"int_memory\": int(data_list[6]),\n",
        "        \"m_dep\": float(data_list[7]),\n",
        "        \"mobile_wt\": int(data_list[8]),\n",
        "        \"n_cores\": int(data_list[9]),\n",
        "        \"pc\": int(data_list[10]),\n",
        "        \"px_height\": int(data_list[11]),\n",
        "        \"px_width\": int(data_list[12]),\n",
        "        \"ram\": int(data_list[13]),\n",
        "        \"sc_h\": int(data_list[14]),\n",
        "        \"sc_w\": int(data_list[15]),\n",
        "        \"talk_time\": int(data_list[16]),\n",
        "        \"three_g\": int(data_list[17]),\n",
        "        \"touch_screen\": int(data_list[18]),\n",
        "        \"wifi\": int(data_list[19])\n",
        "    }\n",
        "\n",
        "    data_df = spark.createDataFrame([data_dict])\n",
        "\n",
        "    # Применение модели\n",
        "    predictions = model.transform(data_df)\n",
        "\n",
        "    # Показать результаты\n",
        "    predictions.select(\"prediction\", \"features\").show()\n",
        "\n"
      ],
      "metadata": {
        "id": "feq8qRTS0HIy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_zip_path = 'lr_model.zip'\n",
        "data_string = \"842,0,2.2,0,1,0,7,0.6,188,2,2,20,756,2549,9,7,19,0,0,1\"  # Данные в виде строки\n",
        "apply_model_to_data_string(model_zip_path, data_string)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctcE92Qw0Kj8",
        "outputId": "67feeaa7-ce8d-413c-e4da-4bf5d9f29167"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+\n",
            "|prediction|            features|\n",
            "+----------+--------------------+\n",
            "|       1.0|[842.0,0.0,2.2,0....|\n",
            "+----------+--------------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}